// Publications file
// ---Expected Format--- 
// - Conference Article: 0|<Publication Title>|<Year of Publication>|<Audience Size>|<Conference Name>|<Conference Place>|<Conference Date>
// <Summary>
// <Keywords>
// <Author Investigators ID List>
// - Magazine Article: 1|<Publication Title>|<Year of Publication>|<Audience Size>|<Magazine Name>|<Magazine Number>|<Magazine Date>
// <Summary>
// <Keywords>
// <Author Investigators ID List>
// - Book: 2|<Publication Title>|<Year of Publication>|<Audience Size>|<ISBN>|<Publishing Company>
// <Summary>
// <Keywords>
// <Author Investigators ID List>
// - Book Chapter: 3|<Publication Title>|<Year of Publication>|<Audience Size>|<ISBN>|<Publishing Company>|<Chapter Name>|<Begin Page>|<End Page>
// <Summary>
// <Keywords>
// <Author Investigators ID List>
// - Conference Article Book: 0|<Publication Title>|<Year of Publication>|<Audience Size>|<ISBN>|<Publishing Company>|<Conference Name>|<Number of Articles>
// <Summary>
// <Keywords>
// <Author Investigators ID List>

0|Modelling web-service uncertainty: The angel/daemon approach|2018|600|WWW : International World Wide Web Conferences (WWW)|Ljubljana, Slovenia|13/03/2018
A survey of our joint research work with Maria Serna on uncertain systems is presented in this paper. The study was originally motivated by a wish to understand the behaviour of a large web application when some of its component parts were subject to external attack (resulting in component degradation or even failure). In the absence of a priori failure information it is possible to constrain system behaviour between the best and worst possible outcomes. Uncertain, multi-component systems can be modelled by orchestrations; an orchestration is a computation which operates in a globally-accessible, multi-user environment. When executed it calls a number of web-services and co-ordinates their responses. The behaviour of computations in web-environments can be highly uncertain: external web-services may be subject to periodic denial of service attacks and network failures, resulting in delayed, or even null, responses. The reliability of an orchestration can be measured by assessing either the completeness of its behaviour (the number of computed results) or its response time (execution time). The behaviour of a system is assessed prior to execution (ex-ante) by modelling an unreliable web-environment as a parameterised uncertainty profile . The parameters of bound the extent of behavioural damage that can occur to an orchestration in a faulty environment. Informally speaking provides a blurred snapshot of operating conditions. An uncertainty profile can be assessed using a two player angel/daemon () zero-sum game : player is motivated to maximise the behaviour of the orchestration whereas player has the opposite intent. Uncertainty profiles have been applied to design a resilient matrix-product cloud application. The approach has also been applied to situations in social sciences, such as short-time macroeconomics (the IS-LM model) and voting games.
Uncertainty|Uncertainty profiles|Zero-sum games|Angel/daemon-games|Matrix product|Short-time macroeconomic models|Voting games
1|2|23|18|10

0|Ensemble learning for intrusion detection systems: A systematic mapping study and cross-benchmark evaluation|2019|7028|USENIX Security : USENIX Security Symposium|Vancouver, Canada|15/07/2019
The security of computer networks plays a strategic role in modern computer systems. In order to enforce high protection levels against threats, a number of software tools are currently developed. Intrusion Detection Systems aim at detecting intruder who eluded the "first line" protection. In this paper, a pattern recognition approach to network intrusion detection based on ensemble learning paradigms is proposed. The potentialities of such an approach for data fusion and some open issues are outlined.
Intrusion Detection|pattern classification|ensemble learning|data fusion
5|10|26|18|20

0|Some results of Maria Serna on strategic games: Complexity of equilibria and models|2017|150|IJCNN : International Joint Conference on Neural Networks|Shenzhen, China|20/05/2016
We survey some recent work of Maria Serna concerning strategic games. We first examine some results related to the formal representation of strategic games and the complexity of problems related to both the existence of equilibria and the equivalence of games. These results provided a better understanding of how the succinctness of the game representation affects the complexity of such problems. We then describe some work dealing with the definition of new strategic games. The study of a Network Formation Game with asymmetric players, and the introduction of Celebrity Games as well as of Strategic Search Games intend to explain the natural behaviour of agents acting in some particular environments like communication networks, social networks, or even networks that contain valuable items hidden in different nodes.
Strategic games|Best response|Social optimum|Nash equilibria|Price of anarchy|Complexity Classes
8|12|16|13|6|5

0|Maximum cliques in graphs with small intersection number and random intersection graphs|2016|500|SODA : ACM SIAM Symposium on Discrete Algorithms|Alexandria, United States|10/11/2004
In this paper, we relate the problem of finding a maximum clique to the intersection number of the input graph (i.e. the minimum number of cliques needed to edge cover the graph). In particular, we consider the maximum clique problem for graphs with small intersection number and random intersection graphs (a model in which each one of m labels is chosen independently with probability p by each one of n vertices, and there are edges between any vertices with overlaps in the labels chosen). We first present a simple algorithm which, on input G finds a maximum clique in O(22m+O(m)+n2min{2m,n}) time steps, where m is an upper bound on the intersection number and n is the number of vertices. Consequently, when m ≤ ln ln n the running time of this algorithm is polynomial. We then consider random instances of the random intersection graphs model as input graphs. As our main contribution, we prove that, when the number of labels is not too large (m = n α , 0 < α < 1), we can use the label choices of the vertices to find a maximum clique in polynomial time whp. The proof of correctness for this algorithm relies on our Single Label Clique Theorem, which roughly states that whp a “large enough” clique cannot be formed by more than one label. This theorem generalizes and strengthens other related results in the state of the art, but also broadens the range of values considered (see e.g. [22] and [4]). As an important consequence of our Single Label Clique Theorem, we prove that the problem of inferring the complete information of label choices for each vertex from the resulting random intersection graph (i.e. the label representation of the graph) is solvable whp; namely, the maximum likelihood estimation method will provide a unique solution (up to permutations of the labels). Finding efficient algorithms for constructing such a label representation is left as an interesting open problem for future research.
Bipartite Graph|Random Graph|Intersection Number|Maximum Clique|Input Graph 
1|5|10|15|6|27

0|Distributed denial of service attacks in cloud: State-of-the-art of scientific and commercial solutions|2020|4182|CCS : ACM Symposium on Computer and Communications Security|Orlando , United States|11/12/1999
Cloud computing model provides on demand, elastic and fully managed computer system resources and services to organizations. However, attacks on cloud components can cause inestimable losses to cloud service providers and cloud users. One such category of attacks is the Distributed Denial of Service (DDoS), which can have serious consequences including impaired customer experience, service outage and in severe cases, complete shutdown and total economic unsustainability. Advances in Internet of Things (IoT) and network connectivity have inadvertently facilitated launch of DDoS attacks which have increased in volume, frequency and intensity. Recent DDoS attacks involving new attack vectors and strategies, have precipitated the need for this survey. In this survey, we mainly focus on finding the gaps, as well as bridging those gaps between the future potential DDoS attacks and state-of-the-art scientific and commercial DDoS attack defending solutions. It seeks to highlight the need for a comprehensive detection approach by presenting the recent threat landscape and major cloud attack incidents, estimates of future DDoS, illustrative use cases, commercial DDoS solutions, and the laws governing DDoS attacks in different nations. An up-to-date survey of DDoS detection methods, particularly anomaly based detection, available research tools, platforms and datasets, has been given. This paper further explores the use of machine learning methods for detection of DDoS attacks and investigates features, strengths, weaknesses, tools, datasets, and evaluates results of the methods in the context of the cloud. A summary comparison of statistical, machine learning and hybrid methods has been brought forth based on detailed analysis. This paper is intended to serve as a ready reference for the research community to develop effective and innovative detection mechanisms for forthcoming DDoS attacks in the cloud environment. It will also sensitize cloud users and providers to the urgent need to invest in deployment of DDoS detection mechanisms to secure their assets.
Anomaly based detection|Cloud computing|DDoS attack|Economic denial of sustainability|Machine learning|Deep learning|Statistical methods
1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28

1|Heuristic alternatives to DP aproaches|2017|5000|WIRED Magazine|113242142|27/10/2015
In short-term planning of a hydropower system one optimizes the choice of release of water and which generators to use. This problem is prone to Bellman’s curse of dimensionality, since the amount of states increases exponentially with the amount of hydropower stations in the system. In this thesis, two different dynamic programming heuristics to this problem are derived, described and compared with a heuristic developed by Powel AS. A difficulty that extends the problem, is the addition of a power reserve on each time period. A third dynamic programming heuristic is developed to deal with a constraint on the power reserve and the parallel with knapsack problem is described. The new dynamic programming approaches perform similar to the heuristic by Powel AS. The third dynamic programming heuristic that deals with the reserve constraints, provides a better way of dealing with and fulfilling the constraint than currently in place by Powel AS. 
Heuristics|Dynamic Programming|Bellman's curse
1|2|3|5|6

1|How Quantum Computing will allow solving the P vs NP problem|2019|2000|GeekTime|1328597|25/10/2018
he P versus NP problem is a major unsolved problem in computer science. It asks whether every problem whose solution can be quickly verified can also be solved quickly. It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute, each of which carries a US$1,000,000 prize for the first correct solution. The informal term quickly, used above, means the existence of an algorithm solving the task that runs in polynomial time, such that the time to complete the task varies as a polynomial function on the size of the input to the algorithm (as opposed to, say, exponential time). The general class of questions for which some algorithm can provide an answer in polynomial time is called "class P" or just "P". For some questions, there is no known way to find an answer quickly, but if one is provided with information showing what the answer is, it is possible to verify the answer quickly. The class of questions for which an answer can be verified in polynomial time is called NP, which stands for "nondeterministic polynomial time"
Quantum Computing|P vs NP|Polynomial Complexity|Computer Science
1|4|14|20|19

1|Quantum entanglement and instant data transmission|2019|3000|MIT Technology Review|499284|28/02/2017
Quantum entanglement is a physical phenomenon that occurs when a pair or group of particles is generated, interact, or share spatial proximity in a way such that the quantum state of each particle of the pair or group cannot be described independently of the state of the others, including when the particles are separated by a large distance. The topic of quantum entanglement is at the heart of the disparity between classical and quantum physics: entanglement is a primary feature of quantum mechanics lacking in classical mechanics.According to some interpretations of quantum mechanics, the effect of one measurement occurs instantly. Other interpretations which don't recognize wavefunction collapse dispute that there is any "effect" at all. However, all interpretations agree that entanglement produces correlation between the measurements and that the mutual information between the entangled particles can be exploited, but that any transmission of information at faster-than-light speeds is impossible
Quantum entanglement|Wavefunction|Instant|Correlation
4|10|26|27|28|5

1|Ransomware Is Headed Down a Dire Path|2018|6000|WIRED|32879.23|26/03/2018
2020 was a great year for ransomware gangs. For hospitals, schools, municipal governments, and everyone else, it’s going to get worse before it gets better.
Ransomware
26|27|28

1|The architecture of virtual machines|2020|5000|Innovation & Tech Today|4122-84|28/02/2020
A virtual machine can support individual processes or a complete system depending on the abstraction level where virtualization occurs. Some VMs support flexible hardware usage and software isolation, while others translate from one instruction set to another. Virtualizing a system or component -such as a processor, memory, or an I/O device - at a given abstraction level maps its interface and visible resources onto the interface and resources of an underlying, possibly different, real system. Consequently, the real system appears as a different virtual system or even as multiple virtual systems. Interjecting virtualizing software between abstraction layers near the HW/SW interface forms a virtual machine that allows otherwise incompatible subsystems to work together. Further, replication by virtualization enables more flexible and efficient and efficient use of hardware resources.
Virtual machining|Computer architecture|Voice mail|Virtual manufacturing|Computer interfaces|Hardware|Application software|Instruction sets|Operating systems|Microprocessors
3|4|10|24|25

2|Structure and Interpretation of Computer Programs|2017|80000|0262510871|MIT Press
With an analytical and rigorous approach to problem solving and programming techniques, this book is oriented toward engineering. Structure and Interpretation of Computer Programs emphasizes the central role played by different approaches to dealing with time in computational models. Its unique approach makes it appropriate for an introduction to computer science courses, as well as programming languages and program design. Table of contents 1 Building Abstractions with Procedures 2 Building Abstractions with Data 3 Modularity, Objects, and State 4 Metalinguistic Abstraction 5 Computing with Register Machines.
Software and its engineering|Software notations and tools|General programming languages|Language features|Data types|Structures
3|6|10|27|26|23

3|Structure and Interpretation of Computer Programs - Chp. 1|2018|5000|0590797263|MIT Press|Building Abstractions with Procedures|1|100
We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells. A computational process is indeed much like a sorcerer's idea of a spirit. It cannot be seen or touched. It is not composed of matter at all. However, it is very real. It can perform intellectual work. It can answer questions. It can affect the world by disbursing money at a bank or by controlling a robot arm in a factory. The programs we use to conjure processes are like a sorcerer's spells. They are carefully composed from symbolic expressions in arcane and esoteric programming languages that prescribe the tasks we want our processes to perform. A computational process, in a correctly working computer, executes programs precisely and accurately. Thus, like the sorcerer's apprentice, novice programmers must learn to understand and to anticipate the consequences of their conjuring. Even small errors (usually called bugs or glitches) in programs can have complex and unanticipated consequences. Fortunately, learning to program is considerably less dangerous than learning sorcery, because the spirits we deal with are conveniently contained in a secure way. Real-world programming, however, requires care, expertise, and wisdom. A small bug in a computer-aided design program, for example, can lead to the catastrophic collapse of an airplane or a dam or the self-destruction of an industrial robot. Master software engineers have the ability to organize programs so that they can be reasonably sure that the resulting processes will perform the tasks intended. They can visualize the behavior of their systems in advance. They know how to structure programs so that unanticipated problems do not lead to catastrophic consequences, and when problems do arise, they can debug their programs. Well-designed computational systems, like well-designed automobiles or nuclear reactors, are designed in a modular manner, so that the parts can be constructed, replaced, and debugged separately.
Abstractions|Procedures|Programming language
5|11|18|29

3|Structure and Interpretation of Computer Programs - Chp. 2|2018|7000|0590797263|MIT Press|Building Abstractions with Data|100|170
We concentrated in chapter 1 on computational processes and on the role of procedures in program design. We saw how to use primitive data (numbers) and primitive operations (arithmetic operations), how to combine procedures to form compound procedures through composition, conditionals, and the use of parameters, and how to abstract procedures by using define. We saw that a procedure can be regarded as a pattern for the local evolution of a process, and we classified, reasoned about, and performed simple algorithmic analyses of some common patterns for processes as embodied in procedures. We also saw that higher-order procedures enhance the power of our language by enabling us to manipulate, and thereby to reason in terms of, general methods of computation. This is much of the essence of programming.
Abstractions|Data|Processes|Composition|Conditionals
1|14|20|6|24

4|IEEE Data conventions|2017|7000|9973827155|MIT Press|2020 7th Swiss Conference on Data Science (SDS)|11
Learning the basics of a modeling technique is not the same as learning how to use and apply it. To develop a data model of an organization is to gain insights into its nature that do not come easily. Indeed, analysts are often expected to understand subtleties of an organization's structure that may have evaded people who have worked there for years. Here's help for those analysts who have learned the basics of data modeling (or "entity/relationship modeling") but who need to obtain the insights required to prepare a good model of a real business.
The Enterprise and Its World|The Things of the Enterprise|Procedures and Activities|Contracts|Accounting|The Laboratory|Material Requirements Planning|Process Manufacturing|Documents|LowerLevel Conventions
1|5|20|21|6|7|8

4|IEEE Since 2000|2020|500|2916330747|IEEE|IEEE GLOBECOM 2020|100
Conference Article Book containing articles concerning all topics debated in IEEE conferences since 2000
IEEE|2000
1|13|17|23|22|24|25